{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree from Scratch üå≥\n",
    "\n",
    "## Overview \n",
    "In this project, I‚Äôm implementing a simple Decision Tree model using Python and NumPy. This model works for both classification and regression tasks by splitting the data at each node based on a feature that best separates the target variable.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Decision Tree**: A model that splits data into branches based on feature values to make predictions.\n",
    "  \n",
    "- **Entropy**: A measure of impurity or disorder in the data, used to determine the best feature for splitting.\n",
    "  \n",
    "- **Information Gain**: The reduction in entropy achieved by partitioning the dataset based on a feature.\n",
    "  \n",
    "- **Recursion**: A method used to build the tree by repeating the process of splitting until a stopping condition is met.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective üéØ\n",
    "The goal of this project is to:\n",
    "1. Implement a custom Decision Tree class for classification tasks.\n",
    "   \n",
    "2. Use entropy and information gain to split the dataset at each node.\n",
    "   \n",
    "3. Build the decision tree recursively to make predictions for unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision Tree Explanation üß†\n",
    "\n",
    "<img src=\"./figures/decision tree.png\" alt=\"decision tree\" width=\"800\" hight= \"400\"/>\n",
    "\n",
    "\n",
    "### Decision Tree Structure\n",
    "A decision tree is a flowchart-like structure where:\n",
    "- Each internal node represents a \"test\" on a feature (e.g., \"Is feature X ‚â§ value?\").\n",
    "  \n",
    "- Each branch represents the outcome of the test.\n",
    "  \n",
    "- Each leaf node represents a class label (for classification) or a predicted value (for regression).\n",
    "\n",
    "### Entropy and Information Gain\n",
    "\n",
    "**Entropy** is a measure of the disorder or impurity in a set of data. The goal of building a decision tree is to reduce this impurity. The **Information Gain** is the reduction in entropy after a split is made.\n",
    "\n",
    "#### Entropy Formula:\n",
    "$$\n",
    "\\text{Entropy}(S) = -p_1 \\log_2 p_1 - p_2 \\log_2 p_2 - \\dots - p_n \\log_2 p_n\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( p_i \\) is the probability of each class in the dataset.\n",
    "\n",
    "#### Information Gain Formula:\n",
    "$$\n",
    "\\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( S \\) is the original dataset.\n",
    "- \\( A \\) is a feature.\n",
    "- \\( S_v \\) is the subset of the dataset where feature \\( A \\) has value \\( v \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Recursive Tree Building üå≥\n",
    "\n",
    "To build the decision tree, we use recursion:\n",
    "1. **Split the dataset**: At each step, find the feature that maximizes information gain.\n",
    "\n",
    "2. **Create child nodes**: Recursively build child nodes by splitting the data until a stopping condition is met.\n",
    "   \n",
    "3. **Stopping conditions**: \n",
    "   - All data points in a node belong to the same class (for classification) or have the same value (for regression).\n",
    "   - Maximum tree depth is reached.\n",
    "   - The number of samples in the node is too small.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation üõ†Ô∏è\n",
    "\n",
    "Below is the code for implementing a Decision Tree from scratch. The `DecisionTree` class includes methods to:\n",
    "\n",
    "1. **Fit the model**: Build the decision tree by recursively splitting the dataset based on the feature that maximizes information gain.\n",
    "   \n",
    "2. **Predict**: Make predictions by traversing the decision tree from the root to the leaves.\n",
    "   \n",
    "3. **Calculate Entropy and Information Gain**: Compute the entropy and information gain for choosing the best feature for splitting.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code the decision tree from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        \"\"\"\n",
    "        Initialize the Decision Tree Classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        max_depth (int): Maximum depth of the tree. If None, the tree will grow until all leaves are pure or no further splits are possible.\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None  # To store the constructed decision tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Decision Tree on the provided dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        X (numpy array): Feature matrix of shape (n_samples, n_features).\n",
    "        y (numpy array): Target vector of shape (n_samples,).\n",
    "        \"\"\"\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        \"\"\"\n",
    "        Recursively build the decision tree.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy array): Feature matrix at the current node.\n",
    "        y (numpy array): Target vector at the current node.\n",
    "        depth (int): Current depth of the tree.\n",
    "\n",
    "        Returns:\n",
    "        dict: A dictionary representing a node in the decision tree.\n",
    "        \"\"\"\n",
    "        # Base case: Stop if all labels are the same or maximum depth is reached\n",
    "        if len(np.unique(y)) == 1 or (self.max_depth and depth >= self.max_depth):\n",
    "            return {\"class\": np.bincount(y).argmax()}  # Majority class at this node\n",
    "\n",
    "        # Find the best feature and threshold to split the data\n",
    "        best_split = self._find_best_split(X, y)\n",
    "\n",
    "        # Recursively build the left and right subtrees\n",
    "        left_tree = self._build_tree(*best_split[\"left\"], depth + 1)\n",
    "        right_tree = self._build_tree(*best_split[\"right\"], depth + 1)\n",
    "\n",
    "        # Return the current node with split information and children\n",
    "        return {\n",
    "            \"feature\": best_split[\"feature\"],  # Index of the feature to split on\n",
    "            \"threshold\": best_split[\"threshold\"],  # Value of the feature for the split\n",
    "            \"left\": left_tree,  # Left subtree\n",
    "            \"right\": right_tree,  # Right subtree\n",
    "        }\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"\n",
    "        Find the best feature and threshold to split the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy array): Feature matrix.\n",
    "        y (numpy array): Target vector.\n",
    "\n",
    "        Returns:\n",
    "        dict: Information about the best split, including the feature, threshold, and resulting subsets.\n",
    "        \"\"\"\n",
    "        best_gain = -1  # Initialize the best information gain\n",
    "        best_split = {}  # Store the details of the best split\n",
    "        n_features = X.shape[1]  # Number of features\n",
    "\n",
    "        # Loop through each feature\n",
    "        for feature in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature])  # Unique values of the feature\n",
    "            for threshold in thresholds:\n",
    "                # Create masks for left and right splits\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                left_y, right_y = y[left_mask], y[right_mask]\n",
    "\n",
    "                # Calculate the information gain of this split\n",
    "                gain = self._information_gain(y, left_y, right_y)\n",
    "                if gain > best_gain:  # Update best split if this one is better\n",
    "                    best_gain = gain\n",
    "                    best_split = {\n",
    "                        \"feature\": feature,  # Best feature index\n",
    "                        \"threshold\": threshold,  # Best threshold value\n",
    "                        \"left\": (X[left_mask], left_y),  # Left split\n",
    "                        \"right\": (X[right_mask], right_y),  # Right split\n",
    "                    }\n",
    "\n",
    "        return best_split\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculate the entropy of a dataset.\n",
    "\n",
    "        Parameters:\n",
    "        y (numpy array): Target vector.\n",
    "\n",
    "        Returns:\n",
    "        float: Entropy value.\n",
    "        \"\"\"\n",
    "        probs = np.bincount(y) / len(y)  # Class probabilities\n",
    "        return -np.sum(p * np.log2(p) for p in probs if p > 0)  # Entropy formula\n",
    "\n",
    "    def _information_gain(self, parent, left, right):\n",
    "        \"\"\"\n",
    "        Calculate the information gain from a split.\n",
    "\n",
    "        Parameters:\n",
    "        parent (numpy array): Original dataset target vector.\n",
    "        left (numpy array): Target vector for the left split.\n",
    "        right (numpy array): Target vector for the right split.\n",
    "\n",
    "        Returns:\n",
    "        float: Information gain value.\n",
    "        \"\"\"\n",
    "        parent_entropy = self._entropy(parent)  # Entropy of the parent node\n",
    "        n = len(parent)  # Total number of samples\n",
    "        left_weight = len(left) / n  # Proportion of samples in the left split\n",
    "        right_weight = len(right) / n  # Proportion of samples in the right split\n",
    "\n",
    "        # Information gain formula\n",
    "        return parent_entropy - (left_weight * self._entropy(left) + right_weight * self._entropy(right))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input data.\n",
    "\n",
    "        Parameters:\n",
    "        X (numpy array): Feature matrix of shape (n_samples, n_features).\n",
    "\n",
    "        Returns:\n",
    "        numpy array: Predicted class labels.\n",
    "        \"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.tree) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x, tree):\n",
    "        \"\"\"\n",
    "        Traverse the decision tree to make a prediction for a single sample.\n",
    "\n",
    "        Parameters:\n",
    "        x (numpy array): Single feature vector.\n",
    "        tree (dict): The current node of the decision tree.\n",
    "\n",
    "        Returns:\n",
    "        int: Predicted class label.\n",
    "        \"\"\"\n",
    "        if \"class\" in tree:  # If it's a leaf node, return the class\n",
    "            return tree[\"class\"]\n",
    "\n",
    "        # Determine which subtree to traverse based on the threshold\n",
    "        feature_val = x[tree[\"feature\"]]\n",
    "        if feature_val <= tree[\"threshold\"]:\n",
    "            return self._traverse_tree(x, tree[\"left\"])  # Go to the left subtree\n",
    "        else:\n",
    "            return self._traverse_tree(x, tree[\"right\"])  # Go to the right subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [1 1 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qg/mkzrt26d3w3g4nybmsfxhqhm0000gn/T/ipykernel_28360/652398911.py:103: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return -np.sum(p * np.log2(p) for p in probs if p > 0)  # Entropy formula\n"
     ]
    }
   ],
   "source": [
    "# Sample dataset\n",
    "# X: Feature matrix where each row is a sample and each column is a feature\n",
    "# y: Target vector with class labels (binary in this case)\n",
    "X = np.array([[1, 1], [1, 0], [0, 1], [0, 0]])  # Feature matrix\n",
    "y = np.array([1, 1, 0, 0])  # Target labels\n",
    "\n",
    "# Initialize the Decision Tree with a maximum depth of 3\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "# Train the Decision Tree on the dataset\n",
    "dt.fit(X, y)\n",
    "\n",
    "# Predict the class labels for the training data\n",
    "predictions = dt.predict(X)\n",
    "\n",
    "# Output the predictions\n",
    "print(\"Predictions:\", predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use Decision Tree üå≥\n",
    "\n",
    "Decision trees are versatile machine learning models widely used for both classification and regression tasks. Here are some scenarios where decision trees are particularly effective:\n",
    "\n",
    "- **Predicting Categorical Outcomes**: Decision trees are ideal for predicting class labels in classification tasks (e.g., spam vs. non-spam, customer churn).  \n",
    "  \n",
    "- **Predicting Continuous Outcomes**: Decision trees can also be used in regression tasks to predict continuous values (e.g., predicting house prices, sales forecasts). \n",
    "   \n",
    "- **Handling Non-linear Relationships**: Decision trees can handle non-linear relationships between features and the target variable without requiring complex transformations. \n",
    "   \n",
    "- **Clear Interpretability**: Decision trees are highly interpretable and easy to visualize, making it straightforward to understand how the model makes decisions. \n",
    "   \n",
    "- **Feature Importance**: Decision trees help identify which features are most important for making predictions, providing valuable insights into the data.\n",
    "\n",
    "# Pros of Decision Trees ‚úÖ\n",
    "\n",
    "- **Easy to Interpret üìñ**: The decision tree structure is intuitive and easy to visualize, making it easy to understand how decisions are made.  \n",
    "  \n",
    "- **Handles Both Classification and Regression**: Decision trees can be used for both classification and regression tasks, making them versatile for different types of problems.\n",
    "    \n",
    "- **No Need for Feature Scaling**: Unlike many other models, decision trees do not require features to be scaled or normalized.  \n",
    "  \n",
    "- **Works Well with Non-linear Data**: Decision trees can capture non-linear relationships between features and the target variable, which makes them more flexible than linear models.  \n",
    "  \n",
    "- **Handles Missing Data üîç**: Decision trees can handle missing data effectively in some implementations by using surrogate splits or ignoring missing values during splitting.\n",
    "\n",
    "# Cons of Decision Trees ‚ùå\n",
    "\n",
    "- **Overfitting üòì**: Decision trees can easily overfit the data, especially if the tree is too deep. Pruning, cross-validation, or ensemble methods like Random Forest can help mitigate this.\n",
    "    \n",
    "    <img src=\"./figures/overfit.png\" alt=\"gradient\" width=\"900\" hight= \"400\"/> \n",
    "\n",
    "    \n",
    "- **Instability ‚ö†Ô∏è**: Small changes in the data can result in a completely different tree, leading to instability. Ensemble methods like Random Forests can address this.  \n",
    "  \n",
    "- **Bias with Imbalanced Data ‚öñÔ∏è**: Decision trees can be biased towards the majority class in imbalanced datasets. Techniques like class weights, oversampling, or using metrics like F1-score can help. \n",
    "   \n",
    "- **Can be Greedy**: Decision trees use a greedy algorithm to split nodes, which may not always lead to the globally optimal tree structure.  \n",
    "  \n",
    "- **Prone to High Variance**: Without proper tuning, decision trees can have high variance, meaning they may perform well on training data but poorly on unseen data.\n",
    "\n",
    "# Ensemble Methods üåü\n",
    "\n",
    "Random Forest and Gradient Boosted Trees are powerful ensemble methods based on decision trees. They address weaknesses like overfitting and instability by combining multiple trees to improve robustness and accuracy.\n",
    "\n",
    "## Conclusion üéØ\n",
    "\n",
    "Decision trees are powerful, interpretable models that can be used for both classification and regression tasks. Their ability to handle non-linear data, provide insights into feature importance, and visualize decisions makes them highly effective. However, care must be taken to avoid overfitting and instability, which can be mitigated using techniques like pruning, cross-validation, or ensemble methods like Random Forests and Gradient Boosted Trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
