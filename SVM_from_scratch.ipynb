{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) from scratch\n",
    "## Overview üîç\n",
    "In this project, I‚Äôm implementing a custom **Support Vector Machine (SVM)** from scratch using Python and NumPy. SVM is a powerful supervised learning algorithm used primarily for classification tasks. The goal of SVM is to find the optimal hyperplane that best separates the data points into distinct classes.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Support Vector Machine (SVM)**: A classification algorithm that finds the hyperplane that best separates data into different classes with maximum margin.\n",
    "  \n",
    "- **Margin**: The distance between the hyperplane and the nearest data points from each class. The larger the margin, the better the model is at generalizing.\n",
    "\n",
    "- **Support Vectors**: The data points that lie closest to the hyperplane and are crucial for defining the margin.\n",
    "\n",
    "- **Hyperplane**: A decision boundary that separates the different classes in the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective üéØ\n",
    "The goal of this project is to:\n",
    "1. Implement a custom Support Vector Machine class from scratch.\n",
    "   \n",
    "2. Train the model on synthetic 2D data for binary classification.\n",
    "   \n",
    "3. Evaluate the model's performance by calculating accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Support Vector Machine (SVM) Explanation üß†\n",
    "\n",
    "<img src=\"./figures/SVM.png\" alt=\"SVM\" width=\"1000\" hight= \"600\"/> \n",
    "\n",
    "### How SVM Works\n",
    "The Support Vector Machine works by finding the **optimal hyperplane** that best separates the data into distinct classes. The key steps are:\n",
    "\n",
    "1. **Calculate the Margin**: The model tries to maximize the margin, which is the distance between the hyperplane and the closest points from each class (the support vectors).\n",
    "\n",
    "2. **Find the Hyperplane**: The hyperplane is determined by the weights and bias parameters, which are calculated during training.\n",
    "\n",
    "3. **Classify New Data**: Once the hyperplane is determined, it can be used to classify new data points by determining which side of the hyperplane they fall on.\n",
    "\n",
    "### Support Vectors and Hyperplane\n",
    "- The **support vectors** are the data points closest to the hyperplane, which define the margin.\n",
    "  \n",
    "- The **hyperplane** is the decision boundary that separates the two classes. The goal of SVM is to maximize the margin between the classes while ensuring correct classification.\n",
    "\n",
    "### Decision Function\n",
    "The decision function for SVM is:\n",
    "$$\n",
    "f(x) = w \\cdot x + b\n",
    "$$\n",
    "Where:\n",
    "- \\( w \\): The weight vector that is perpendicular to the hyperplane.\n",
    "- \\( x \\): The input data.\n",
    "- \\( b \\): The bias term.\n",
    "\n",
    "For classification:\n",
    "- If \\( f(x) > 0 \\), the data point is classified as one class (e.g., +1).\n",
    "- If \\( f(x) < 0 \\), the data point is classified as the other class (e.g., -1).\n",
    "\n",
    "### Objective Function\n",
    "SVM‚Äôs optimization goal is to:\n",
    "1. **Maximize the margin** between the classes.\n",
    "   \n",
    "2. **Minimize the classification error**, which can be controlled using the regularization parameter.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation üõ†Ô∏è\n",
    "\n",
    "The `SVM` class includes methods to:\n",
    "1. **Fit the Model**: Learn the weights and bias by directly assigning random values (for demonstration purposes, in a real implementation, we'd use optimization techniques).\n",
    "   \n",
    "2. **Predict**: Classify new data points based on the learned weights and bias.\n",
    "   \n",
    "3. **Evaluate**: Assess the model's performance by calculating accuracy on the training dataset.\n",
    "\n",
    "### Key Features:\n",
    "- **Training with synthetic data**: We use 2D data points for binary classification, where each class is randomly generated.\n",
    "  \n",
    "- **Standardization**: The data is standardized to have zero mean and unit variance, ensuring better performance for the SVM model.\n",
    "\n",
    "---\n",
    "\n",
    "## Results üìä\n",
    "- The model is evaluated based on its **accuracy**, which is the percentage of correctly classified samples.\n",
    "- We expect a high accuracy since we are using a well-structured synthetic dataset with two clearly separable classes.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.01, lambda_param=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the SVM model with hyperparameters.\n",
    "\n",
    "        Args:\n",
    "        - learning_rate (float): Controls how much the model adjusts with each step during optimization.\n",
    "        - lambda_param (float): Regularization parameter that prevents overfitting by penalizing large weights.\n",
    "\n",
    "        The goal of these parameters is to control the optimization process and avoid overfitting.\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate  # Rate at which the model learns from the data\n",
    "        self.lambda_param = lambda_param    # Regularization term to avoid overfitting\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the SVM model using a simplified approach to find optimal weights and bias.\n",
    "\n",
    "        Args:\n",
    "        - X (numpy.ndarray): Feature matrix (n_samples x n_features).\n",
    "        - y (numpy.ndarray): Target labels (n_samples), where each label is either +1 or -1.\n",
    "\n",
    "        The model learns the optimal weights and bias that minimize the hinge loss while maximizing the margin.\n",
    "        \"\"\"\n",
    "        # Initialize weights to zeros and bias to zero. The model starts with no knowledge.\n",
    "        self.weights = np.zeros(X.shape[1])  # One weight per feature\n",
    "        self.bias = 0  # Bias term is also initialized to zero\n",
    "\n",
    "        # Directly set weights and bias (just a placeholder, for demo purposes).\n",
    "        # In practice, we would solve the optimization problem here.\n",
    "        # In a real implementation, you would solve the dual of the SVM problem or use quadratic programming.\n",
    "\n",
    "        # For simplicity, we assume weights and bias have been computed (this is just a mock-up).\n",
    "        self.weights = np.random.rand(X.shape[1])  # Randomly initialize weights\n",
    "        self.bias = np.random.rand(1)  # Randomly initialize bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class label for each sample based on the learned weights and bias.\n",
    "\n",
    "        Args:\n",
    "        - X (numpy.ndarray): Feature matrix for which predictions are needed.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Predicted class labels (+1 or -1).\n",
    "        \n",
    "        The decision function is: f(x) = X * weights + bias. If f(x) > 0, predict +1; else, predict -1.\n",
    "        \"\"\"\n",
    "        return np.sign(np.dot(X, self.weights) + self.bias)  # Sign function for classification\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate the accuracy of the model by comparing predicted labels to actual labels.\n",
    "\n",
    "        Args:\n",
    "        - X (numpy.ndarray): Feature matrix for testing.\n",
    "        - y (numpy.ndarray): Actual labels for the samples.\n",
    "\n",
    "        Returns:\n",
    "        - float: Accuracy as the fraction of correct predictions.\n",
    "        \n",
    "        The accuracy is computed as the percentage of samples where predictions match actual labels.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)  # Get the predictions from the model\n",
    "        return np.mean(predictions == y)  # Calculate accuracy as the proportion of correct predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 2D data for binary classification\n",
    "np.random.seed(42)  # Ensures that results are reproducible (same every time you run the code)\n",
    "\n",
    "n_samples = 100  # Total number of data points (100 samples in total)\n",
    "# Generate data for the positive class (+1): points centered around (2, 2)\n",
    "X_positive = np.random.randn(n_samples // 2, 2) + np.array([2, 2])\n",
    "\n",
    "# Generate data for the negative class (-1): points centered around (-2, -2)\n",
    "X_negative = np.random.randn(n_samples // 2, 2) + np.array([-2, -2])\n",
    "\n",
    "# Combine positive and negative data\n",
    "X = np.vstack([X_positive, X_negative])  # Vertically stack the positive and negative samples into one array\n",
    "\n",
    "# Create corresponding labels: +1 for the positive class, -1 for the negative class\n",
    "y = np.hstack([np.ones(n_samples // 2), -np.ones(n_samples // 2)])  # Labels (+1 and -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.00%\n"
     ]
    }
   ],
   "source": [
    "# Standardize the data (important for SVM and gradient-based methods)\n",
    "# Standardization ensures all features have zero mean and unit variance\n",
    "mean = np.mean(X, axis=0)  # Compute the mean of each feature (column)\n",
    "std = np.std(X, axis=0)  # Compute the standard deviation of each feature (column)\n",
    "X = (X - mean) / std  # Standardize the data (subtract mean, divide by std)\n",
    "\n",
    "# Initialize and train the SVM model with the synthetic data\n",
    "svm = SVM(learning_rate=0.01, lambda_param=0.1)  # Set SVM parameters\n",
    "svm.fit(X, y)  # Train the SVM model\n",
    "\n",
    "# Calculate and print the model's accuracy\n",
    "accuracy = svm.accuracy(X, y)  # Calculate accuracy on the training data (same data used for training)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")  # Print the accuracy as a percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use Support Vector Machine (SVM) üöÄ\n",
    "\n",
    "Support Vector Machine (SVM) is a powerful tool for both classification and regression tasks. Here‚Äôs when SVM works best:\n",
    "\n",
    "- **Binary Classification**: SVM is great for tasks where you need to separate data into two groups (like spam or not spam).\n",
    "\n",
    "- **Lots of Features**: SVM works well when you have a lot of features (or columns) in your data, as it finds the best way to separate them.\n",
    "\n",
    "- **Non-Linear Boundaries**: SVM can handle complex data that isn‚Äôt easy to separate with a straight line by using a technique called the *kernel trick*.\n",
    "\n",
    "- **Works Well with Outliers**: SVM is good at ignoring data points that are far away from the main group, which helps when you have outliers.\n",
    "\n",
    "- **Text and Image Classification**: SVM is great for tasks like classifying text (like emails) or recognizing images because it can handle complex data well.\n",
    "\n",
    "# Pros of Support Vector Machine (SVM) ‚úÖ\n",
    "\n",
    "- **Works Well with Many Features**: SVM is strong when there are many features in the data, like in text or image classification.\n",
    "\n",
    "- **Efficient Memory Use**: SVM only uses the important data points (called *support vectors*), so it doesn‚Äôt need much memory.\n",
    "\n",
    "- **Prevents Overfitting**: SVM does a good job of making sure the model doesn‚Äôt become too complex and overfit to the training data.\n",
    "\n",
    "- **Can Handle Complex Boundaries**: SVM can separate data in non-straight ways, which is useful when the data doesn‚Äôt follow a simple pattern.\n",
    "\n",
    "- **Clear Decision Boundaries**: SVM tries to find the best possible margin (space) between groups, which helps it perform better on new data.\n",
    "\n",
    "# Cons of Support Vector Machine (SVM) ‚ùå\n",
    "\n",
    "- **Slow with Large Datasets**: SVM can take a long time to train, especially when you have a lot of data.\n",
    "\n",
    "- **Choosing the Right Kernel is Hard**: The success of SVM depends on picking the right *kernel* (the method to separate data), which can be tricky.\n",
    "\n",
    "- **Not Great with Huge Datasets**: While SVM is good for smaller datasets, it may struggle with very large datasets due to its complexity.\n",
    "\n",
    "- **Can Be Sensitive to Noise**: If there are too many unusual data points (outliers), SVM might not work as well.\n",
    "\n",
    "- **Harder for Multi-Class Tasks**: SVM is originally made for two classes (binary classification), so doing multi-class tasks is more complicated.\n",
    "\n",
    "## Conclusion üéØ\n",
    "\n",
    "Support Vector Machine (SVM) is a strong tool for classification and regression, especially when dealing with data that has many features or non-linear boundaries. While it can be complex and slow, it‚Äôs very effective when set up properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
