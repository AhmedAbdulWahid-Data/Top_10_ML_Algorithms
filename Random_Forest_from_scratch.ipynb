{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest from Scratch üå≤üå≤üå≤\n",
    "\n",
    "## Overview üìà  \n",
    "In this project, I'm implementing a custom **Random Forest** algorithm from scratch using Python and NumPy. Random Forest is an ensemble learning technique that combines multiple decision trees to improve classification or regression performance by reducing overfitting and increasing accuracy.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Random Forest**: A supervised machine learning algorithm that builds multiple decision trees and combines their predictions through majority voting (for classification) or averaging (for regression).\n",
    "  \n",
    "- **Decision Trees**: A fundamental component of Random Forests. Each tree splits the data into subsets based on feature values to make predictions.\n",
    "  \n",
    "- **Bootstrap Aggregating (Bagging)**: A technique used to create multiple training datasets by randomly sampling with replacement from the original data to train each decision tree in the forest.\n",
    "  \n",
    "- **Majority Voting**: In classification, the Random Forest model predicts the class that is most common among all the trees. In regression, it averages the predictions from all the trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective üéØ  \n",
    "The goal of this project is to:  \n",
    "1. Implement a custom **Random Forest** model using decision trees as base learners.\n",
    "   \n",
    "2. Use **bagging** to train multiple decision trees on different subsets of the data.\n",
    "   \n",
    "3. Combine the predictions from multiple trees using **majority voting** (classification) or **averaging** (regression).\n",
    "   \n",
    "4. Understand the importance of **random feature selection** during tree construction to enhance model performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Random Forest Explanation üß†  \n",
    "\n",
    "### Bootstrapping and Bagging  \n",
    "To train each decision tree in the forest, we create a different training set using **bootstrapping**. This involves sampling with replacement from the original dataset so that each tree gets a slightly different version of the data. Bagging (Bootstrap Aggregating) is used to reduce overfitting and increase the robustness of the model.\n",
    "\n",
    "### Decision Trees  \n",
    "Each decision tree is trained using a subset of features and samples. The decision trees are built by recursively splitting the data into subsets based on feature values, using a criterion like **Gini impurity** (for classification) or **mean squared error** (for regression).\n",
    "\n",
    "### Random Feature Selection  \n",
    "To ensure that each tree is diverse, Random Forests randomly select a subset of features at each split. This prevents any single feature from dominating the model, leading to more accurate and generalized predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### Combining Tree Predictions  \n",
    "Once we have multiple trees in the forest, we combine their predictions to make the final prediction:\n",
    "- **Classification**: We use **majority voting** where the most common class among all the trees is chosen as the final prediction.\n",
    "  \n",
    "- **Regression**: We use **averaging** where the predictions of all trees are averaged to make the final prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation üõ†Ô∏è\n",
    "\n",
    "Below is the code for implementing a **Random Forest** from scratch. The `RandomForest` class includes methods to:  \n",
    "1. **Train Multiple Trees**: Build several decision trees using different bootstrapped datasets.\n",
    "   \n",
    "2. **Predict Using Majority Voting or Averaging**: Combine predictions from all trees for classification or regression.\n",
    "   \n",
    "3. **Handle Random Feature Selection**: At each node split, select a random subset of features to consider, ensuring diversity in the trees.\n",
    "________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Gini Impurity function: Measures how \"impure\" a split is\n",
    "def gini_impurity(y):\n",
    "    class_counts = np.bincount(y)  # Count the occurrences of each class\n",
    "    probabilities = class_counts / len(y)  # Calculate the probability for each class\n",
    "    return 1 - np.sum(probabilities ** 2)  # Gini formula: 1 - sum(p^2) for each class\n",
    "\n",
    "# Best Split function: Finds the best feature and threshold to split on\n",
    "def best_split(X, y):\n",
    "    best_gini = float('inf')  # Start with a very large number for the best Gini score\n",
    "    best_split = None  # To store the best split we find\n",
    "    \n",
    "    n_samples, n_features = X.shape  # Get the number of samples and features\n",
    "    \n",
    "    # Randomly select a subset of features\n",
    "    feature_indices = np.random.choice(range(n_features), size=int(np.sqrt(n_features)), replace=False)\n",
    "    \n",
    "    # Continue with the current splitting logic for the selected subset\n",
    "    # Loop through each feature\n",
    "    for feature_index in feature_indices:\n",
    "        # Sort the data by this feature\n",
    "        sorted_indices = np.argsort(X[:, feature_index])\n",
    "        sorted_X, sorted_y = X[sorted_indices], y[sorted_indices]  # Sort X and y accordingly\n",
    "        \n",
    "        # Loop through possible splits (between every pair of sorted values)\n",
    "        for i in range(1, n_samples):\n",
    "            left_y = sorted_y[:i]  # Left side of the split\n",
    "            right_y = sorted_y[i:]  # Right side of the split\n",
    "            \n",
    "            # Calculate the Gini impurity for this split\n",
    "            gini_left = gini_impurity(left_y)\n",
    "            gini_right = gini_impurity(right_y)\n",
    "            weighted_gini = (len(left_y) * gini_left + len(right_y) * gini_right) / n_samples\n",
    "            \n",
    "            # If this split is better, save it\n",
    "            if weighted_gini < best_gini:\n",
    "                best_gini = weighted_gini\n",
    "                best_split = (feature_index, sorted_X[i-1, feature_index], left_y, right_y)\n",
    "    \n",
    "    return best_split  # Return the best feature index, threshold, and data splits\n",
    "\n",
    "# Decision Tree Classifier: A simple implementation of a decision tree\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth  # Maximum depth to control overfitting\n",
    "        self.tree = None  # This will store the tree structure\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Train the decision tree using the data (X, y)\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        # If all samples belong to the same class, return that class\n",
    "        if len(set(y)) == 1:\n",
    "            return {\"label\": y[0]}  # Single class label\n",
    "        \n",
    "        # If maximum depth is reached, return the most frequent class\n",
    "        if self.max_depth and depth >= self.max_depth:\n",
    "            return {\"label\": np.bincount(y).argmax()}  # Majority class label\n",
    "        \n",
    "        # Find the best feature and threshold to split the data\n",
    "        feature_index, threshold, left_y, right_y = best_split(X, y)\n",
    "        \n",
    "        # If no good split is found, return the most frequent class\n",
    "        if feature_index is None:\n",
    "            return {\"label\": np.bincount(y).argmax()}\n",
    "        \n",
    "        # Recursively build the left and right subtrees\n",
    "        left_tree = self._build_tree(X[X[:, feature_index] <= threshold], left_y, depth + 1)\n",
    "        right_tree = self._build_tree(X[X[:, feature_index] > threshold], right_y, depth + 1)\n",
    "        \n",
    "        # Return the tree structure\n",
    "        return {\n",
    "            \"feature_index\": feature_index,\n",
    "            \"threshold\": threshold,\n",
    "            \"left\": left_tree,\n",
    "            \"right\": right_tree\n",
    "        }\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Predict the class for each sample\n",
    "        return [self._predict_sample(sample, self.tree) for sample in X]\n",
    "    \n",
    "    def _predict_sample(self, sample, tree):\n",
    "        # If we reach a leaf node, return the label\n",
    "        if \"label\" in tree:\n",
    "            return tree[\"label\"]\n",
    "        \n",
    "        # Otherwise, check which side of the tree the sample goes to\n",
    "        feature_value = sample[tree[\"feature_index\"]]\n",
    "        if feature_value <= tree[\"threshold\"]:\n",
    "            return self._predict_sample(sample, tree[\"left\"])\n",
    "        else:\n",
    "            return self._predict_sample(sample, tree[\"right\"])\n",
    "\n",
    "# Random Forest Classifier: An ensemble of Decision Trees\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees=3, max_depth=None):\n",
    "        self.n_trees = n_trees  # Number of trees in the forest\n",
    "        self.max_depth = max_depth  # Maximum depth of each tree\n",
    "        self.trees = []  # This will store all the decision trees\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Train multiple decision trees using the same dataset\n",
    "        for _ in range(self.n_trees):\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.fit(X, y)  # Train each tree on the full dataset\n",
    "            self.trees.append(tree)  # Add the tree to the forest\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Get predictions from all trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        \n",
    "        # For each sample, take the majority vote across all trees\n",
    "        predictions = [np.bincount(predictions).argmax() for predictions in tree_predictions.T]\n",
    "        \n",
    "        # Convert predictions to Python native ints for cleaner output\n",
    "        return [int(pred) for pred in predictions]\n",
    "\n",
    "# Example usage:\n",
    "# Example dataset (X: features, y: labels)\n",
    "X = np.array([[2, 3], [10, 15], [3, 4], [9, 14], [1, 1], [7, 10]])\n",
    "y = np.array([0, 1, 0, 1, 0, 1])\n",
    "\n",
    "# Train a Random Forest\n",
    "rf = RandomForest(n_trees=3, max_depth=3)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Make predictions\n",
    "predictions = rf.predict(X)\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use Random Forest üå≤üå≤üå≤\n",
    "\n",
    "Random Forest is a powerful model that works well in many situations. Here are some cases where it‚Äôs especially useful:\n",
    "\n",
    "- **High Variability in Data**: Random Forest is great for datasets with a lot of different values because it creates many decision trees from random parts of the data, making it more reliable and less likely to overfit.\n",
    "  > - **Overfitting Prevention**: Since it combines the predictions from multiple trees, it helps prevent the model from being too specific to the training data.\n",
    "  \n",
    "  <img src=\"./figures/overfit.png\" alt=\"gradient\" width=\"900\" hight= \"400\"/> \n",
    "\n",
    "- **Complex Data Patterns**: Random Forest can handle both simple and complex relationships between features, making it good for data that is not straightforward.\n",
    "\n",
    "- **Handling Missing Data**: Random Forest can work with missing data by using different subsets of the data to build each tree, which helps fill in gaps.\n",
    "\n",
    "- **Feature Importance**: Random Forest tells you which features (columns) in your data are the most important for making predictions.\n",
    "\n",
    "- **Classification and Regression**: Random Forest can be used for both types of tasks: classifying things into categories and predicting continuous values.\n",
    "\n",
    "---\n",
    "\n",
    "# Pros of Random Forest ‚úÖ\n",
    "\n",
    "- **Better Accuracy**: By combining the results of many trees, Random Forest gives more accurate predictions than using just one tree.\n",
    "\n",
    "- **Stability**: Random Forest is good at handling noisy or uneven data, so it‚Äôs more stable and less likely to make mistakes.\n",
    "\n",
    "- **Works Well with Lots of Features**: You don‚Äôt need to worry much about too many features in your data because Random Forest can handle them well.\n",
    "\n",
    "- **Feature Importance**: It helps you figure out which features are the most important for making predictions, giving you insights into your data.\n",
    "\n",
    "- **Works for Many Types of Problems**: Random Forest works for both predicting categories (classification) and continuous values (regression), making it very flexible.\n",
    "\n",
    "---\n",
    "\n",
    "# Cons of Random Forest ‚ùå\n",
    "\n",
    "- **Complexity**: Random Forest can be slow and require a lot of memory, especially for big datasets.\n",
    "\n",
    "- **Hard to Explain**: While individual decision trees are easy to understand, the combined forest of trees can be hard to explain, which can be a problem in some situations.\n",
    "\n",
    "- **Slower Predictions**: Since it uses many trees to make a decision, predictions can take longer compared to simpler models.\n",
    "\n",
    "- **Bias with Imbalanced Data**: Random Forest might favor the more common class in data that is unbalanced (e.g., too many positive or negative examples).\n",
    "\n",
    "- **Not Ideal for Small Datasets**: For small datasets, simpler models like decision trees or logistic regression might work better than Random Forest.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion üéØüå≤üå≤üå≤\n",
    "\n",
    "Random Forest is a flexible and strong tool that works well for many types of data, especially when dealing with a lot of features, complexity, or noise. While it can be slow and hard to explain, it is great for improving accuracy and handling both classification and prediction tasks. Use it carefully, and it can make your machine learning projects more powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
