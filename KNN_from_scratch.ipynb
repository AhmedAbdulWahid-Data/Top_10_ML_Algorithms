{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) from Scratch\n",
    "\n",
    "## Overview üîç\n",
    "In this project, I‚Äôm implementing a custom **K-Nearest Neighbors (KNN)** algorithm from scratch using Python and NumPy. KNN is a simple yet powerful algorithm that can be used for both classification and regression. It makes predictions by looking at the most similar data points (or \"neighbors\") in the training set and assigning the majority class (for classification) or averaging values (for regression).\n",
    "\n",
    "### Key Concepts:\n",
    "- **K-Nearest Neighbors (KNN)**: A classification and regression method that uses the closest K data points to make a prediction.\n",
    "  \n",
    "- **Distance Calculation**: The algorithm utilizes both **Euclidean distance** and **Manhattan distance** to measure similarity between data points.\n",
    "  \n",
    "- **Hyperparameter \\( k \\)**: The number of neighbors to consider for making a prediction. Choosing \\( k \\) affects the model‚Äôs bias and variance.\n",
    "  \n",
    "- **Decision Boundaries**: The boundaries between different classes created based on the neighbors, providing a visual representation of how the model classifies each region.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective üéØ\n",
    "The goal of this project is to:\n",
    "1. Implement a custom K-Nearest Neighbors class from scratch.\n",
    "2. Allow the model to handle both classification and regression tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## K-Nearest Neighbors Explanation üß†\n",
    "\n",
    "<img src=\"./figures/KNN.png\" alt=\"KNN\" width=\"600\" hight= \"200\"/>\n",
    "\n",
    "\n",
    "### How KNN Works\n",
    "In the K-Nearest Neighbors algorithm, we predict the label of a new data point by finding the **K nearest points** in the training set. The steps to make a prediction are as follows:\n",
    "1. **Calculate Distances**: Measure the distance from the new data point to all points in the training set using both Euclidean and Manhattan distance metrics.\n",
    "   \n",
    "2. **Identify Neighbors**: Select the K closest data points based on the distance.\n",
    "   \n",
    "3. **Make Prediction**: \n",
    "   - For classification, the predicted label is the **majority class** among the neighbors.\n",
    "   - For regression, the prediction is the **average** of the neighbor values.\n",
    "\n",
    "### Distance Calculation\n",
    "To determine the closeness of neighbors, we use the following distance formulas:\n",
    "\n",
    "- **Euclidean Distance**:\n",
    "  $$\n",
    "  \\text{distance} = \\sqrt{\\sum_{i=1}^{n} (x_i - x_{i,\\text{neighbor}})^2}\n",
    "  $$\n",
    "\n",
    "- **Manhattan Distance**:\n",
    "  $$\n",
    "  \\text{distance} = \\sum_{i=1}^{n} |x_i - x_{i,\\text{neighbor}}|\n",
    "  $$\n",
    "\n",
    "Where:\n",
    "- \\( x_i \\): The feature of the data point we want to predict.\n",
    "- \\( x_i,neighbor \\): The corresponding feature of each neighbor.\n",
    "- \\( x \\): Number of features.\n",
    "\n",
    "### Choosing K (Hyperparameter Tuning)\n",
    "The value of **K** affects the model's bias and variance:\n",
    "- **Small K** (e.g., K=1) tends to have low bias and high variance, meaning it can capture more details but might overfit.\n",
    "- **Larger K** values increase bias but reduce variance, leading to a smoother decision boundary and potentially better generalization.\n",
    "\n",
    "### Majority Voting\n",
    "For classification, KNN uses **majority voting** to assign the class label. This means the class with the most occurrences among the K nearest neighbors is chosen as the prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation üõ†Ô∏è\n",
    "\n",
    "The `KNN` class includes methods to:\n",
    "1. **Fit the Model**: Store the training data for distance-based calculations.\n",
    "   \n",
    "2. **Predict**: Find the nearest neighbors for a given input and assign a label based on majority voting (for classification) or average value (for regression).\n",
    "   \n",
    "3. **Evaluate**: Assess the model's performance using accuracy (for classification) or Mean Squared Error (for regression).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the K-Nearest Neighbors (KNN) class\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric='euclidean'):\n",
    "        \"\"\"\n",
    "        Initialize the KNN algorithm with the specified number of neighbors (k) \n",
    "        and the distance metric (either 'euclidean' or 'manhattan').\n",
    "\n",
    "        Parameters:\n",
    "        k (int): The number of nearest neighbors to consider for classification or regression.\n",
    "        distance_metric (str): The type of distance metric to use ('euclidean' or 'manhattan').\n",
    "        \"\"\"\n",
    "        self.k = k  # Set the number of neighbors to use in the algorithm\n",
    "        self.distance_metric = distance_metric  # Set the distance metric ('euclidean' or 'manhattan')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Store the training data and labels for later use in predictions.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): The feature matrix (training data).\n",
    "        y (array-like): The target labels (training labels).\n",
    "        \"\"\"\n",
    "        self.X_train = X  # Store the training features\n",
    "        self.y_train = y  # Store the training labels\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the labels for a set of test samples.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): The feature matrix of test samples to classify.\n",
    "\n",
    "        Returns:\n",
    "        np.array: The predicted labels for each test sample.\n",
    "        \"\"\"\n",
    "        predictions = [self._predict(x) for x in X]  # For each test sample, predict its label\n",
    "        return np.array(predictions)  # Return the list of predictions as a numpy array\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the label for a single test sample 'x'.\n",
    "\n",
    "        Parameters:\n",
    "        x (array-like): A single test sample whose label needs to be predicted.\n",
    "\n",
    "        Returns:\n",
    "        int: The predicted label for the test sample.\n",
    "        \"\"\"\n",
    "        distances = self._compute_distances(x)  # Compute the distances from 'x' to all training samples\n",
    "        k_indices = np.argsort(distances)[:self.k]  # Get indices of the k closest training samples\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]  # Get the labels of the k nearest neighbors\n",
    "        return np.bincount(k_nearest_labels).argmax()  # Return the most frequent label among the k neighbors\n",
    "\n",
    "    def _compute_distances(self, x):\n",
    "        \"\"\"\n",
    "        Compute the distance between the test sample 'x' and all training samples \n",
    "        using the selected distance metric ('euclidean' or 'manhattan').\n",
    "\n",
    "        Parameters:\n",
    "        x (array-like): A test sample for which the distance to each training sample needs to be computed.\n",
    "\n",
    "        Returns:\n",
    "        np.array: An array of distances from 'x' to all training samples.\n",
    "        \"\"\"\n",
    "        if self.distance_metric == 'euclidean':\n",
    "            # Calculate Euclidean distance: sqrt(sum((x_i - x)^2))\n",
    "            distances = np.sqrt(np.sum((self.X_train - x) ** 2, axis=1))\n",
    "        elif self.distance_metric == 'manhattan':\n",
    "            # Calculate Manhattan distance: sum(|x_i - x|)\n",
    "            distances = np.sum(np.abs(self.X_train - x), axis=1)\n",
    "        else:\n",
    "            # Raise an error if an unsupported distance metric is specified\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "        return distances  # Return the calculated distances\n",
    "\n",
    "    def predict_regression(self, X):\n",
    "        \"\"\"\n",
    "        Predict continuous values (regression) for a set of test samples.\n",
    "\n",
    "        Parameters:\n",
    "        X (array-like): The feature matrix of test samples to predict.\n",
    "\n",
    "        Returns:\n",
    "        np.array: The predicted continuous values for each test sample.\n",
    "        \"\"\"\n",
    "        predictions = [self._predict_regression(x) for x in X]  # Use the helper method for each test sample\n",
    "        return np.array(predictions)  # Return the list of predictions as a numpy array\n",
    "\n",
    "    def _predict_regression(self, x):\n",
    "        \"\"\"\n",
    "        Predict the continuous value for a single test sample 'x' in a regression task.\n",
    "\n",
    "        Parameters:\n",
    "        x (array-like): A single test sample whose value needs to be predicted.\n",
    "\n",
    "        Returns:\n",
    "        float: The predicted continuous value for the test sample.\n",
    "        \"\"\"\n",
    "        distances = self._compute_distances(x)  # Compute distances to all training samples\n",
    "        k_indices = np.argsort(distances)[:self.k]  # Get indices of the k closest training samples\n",
    "        k_nearest_values = [self.y_train[i] for i in k_indices]  # Get the continuous values of the k nearest neighbors\n",
    "        return np.mean(k_nearest_values)  # Return the average of the k nearest values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample training data (2 features) and labels for classification\n",
    "X_train = np.array([[1, 2], [2, 3], [3, 4], [6, 7], [7, 8], [8, 9]])\n",
    "y_train_classification = np.array([0, 0, 0, 1, 1, 1])  # Binary classification labels\n",
    "y_train_regression = np.array([1.5, 2.5, 3.5, 6.5, 7.5, 8.5])  # Regression values\n",
    "\n",
    "# Sample test data\n",
    "X_test = np.array([[4, 5], [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Predictions (Euclidean): [0 1]\n"
     ]
    }
   ],
   "source": [
    "# 1. Classification Prediction using Euclidean distance\n",
    "knn_classifier_euclidean = KNN(k=3, distance_metric='euclidean')\n",
    "knn_classifier_euclidean.fit(X_train, y_train_classification)\n",
    "predictions_classification_euclidean = knn_classifier_euclidean.predict(X_test)\n",
    "print(\"Classification Predictions (Euclidean):\", predictions_classification_euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Predictions (Euclidean): [4.16666667 5.83333333]\n"
     ]
    }
   ],
   "source": [
    "# 2. Regression Prediction using Euclidean distance\n",
    "knn_regressor_euclidean = KNN(k=3, distance_metric='euclidean')\n",
    "knn_regressor_euclidean.fit(X_train, y_train_regression)\n",
    "predictions_regression_euclidean = knn_regressor_euclidean.predict_regression(X_test)\n",
    "print(\"Regression Predictions (Euclidean):\", predictions_regression_euclidean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Predictions (Manhattan): [0 1]\n"
     ]
    }
   ],
   "source": [
    "# 3. Classification Prediction using Manhattan distance\n",
    "knn_classifier_manhattan = KNN(k=3, distance_metric='manhattan')\n",
    "knn_classifier_manhattan.fit(X_train, y_train_classification)\n",
    "predictions_classification_manhattan = knn_classifier_manhattan.predict(X_test)\n",
    "print(\"Classification Predictions (Manhattan):\", predictions_classification_manhattan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Predictions (Manhattan): [4.16666667 5.83333333]\n"
     ]
    }
   ],
   "source": [
    "# 4. Regression Prediction using Manhattan distance\n",
    "knn_regressor_manhattan = KNN(k=3, distance_metric='manhattan')\n",
    "knn_regressor_manhattan.fit(X_train, y_train_regression)\n",
    "predictions_regression_manhattan = knn_regressor_manhattan.predict_regression(X_test)\n",
    "print(\"Regression Predictions (Manhattan):\", predictions_regression_manhattan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use KNN üìà\n",
    "\n",
    "K-Nearest Neighbors is a versatile algorithm that can be applied in various scenarios. Here are some situations where KNN is particularly effective:\n",
    "\n",
    "- **Classification Problems**: KNN is primarily used for classification tasks, where the objective is to categorize data points based on their features. It works well in scenarios with clear class boundaries.\n",
    "\n",
    "- **Regression Problems**: While less common, KNN can also be used for regression tasks, predicting continuous values by averaging the outcomes of the K nearest neighbors.\n",
    "\n",
    "- **Small to Medium-Sized Datasets**: KNN performs best with smaller datasets since it relies on distance calculations for each prediction. With larger datasets, the computation can become expensive.\n",
    "\n",
    "- **Low-Dimensional Data**: KNN is sensitive to the curse of dimensionality. It works well when the data has a low number of features, as high-dimensional data can obscure the distance metrics.\n",
    "\n",
    "- **Non-Linear Decision Boundaries**: KNN does not assume any specific distribution of data, making it suitable for problems where decision boundaries are non-linear.\n",
    "\n",
    "# Pros of KNN ‚úÖ\n",
    "\n",
    "- **Simple to Understand and Implement**: KNN is easy to comprehend and implement, making it a good choice for beginners in machine learning.\n",
    "\n",
    "- **No Training Phase**: KNN is a lazy learner; it does not require a training phase. This can be advantageous when the training data changes frequently.\n",
    "\n",
    "- **Versatile**: KNN can be used for both classification and regression tasks.\n",
    "\n",
    "- **Naturally Handles Multi-Class Problems**: KNN can easily classify data into multiple categories without requiring additional adjustments.\n",
    "\n",
    "# Cons of KNN ‚ùå\n",
    "\n",
    "- **Computationally Intensive**: KNN requires calculating distances between the query instance and all training samples, making it slow, especially for large datasets.\n",
    "\n",
    "- **Sensitive to Irrelevant Features**: The presence of irrelevant features can negatively impact the performance of KNN. Feature selection or dimensionality reduction may be needed.\n",
    "\n",
    "- **Curse of Dimensionality**: As the number of dimensions increases, the distance between points becomes less meaningful. This can reduce the effectiveness of KNN in high-dimensional spaces.\n",
    "\n",
    "- **Choosing the Right K**: The performance of KNN is highly dependent on the choice of K (the number of neighbors). Selecting an optimal K value requires cross-validation.\n",
    "\n",
    "- **Memory Intensive**: KNN stores the entire training dataset, which can be a problem for memory-constrained environments.\n",
    "\n",
    "## Conclusion üéØ\n",
    "\n",
    "K-Nearest Neighbors is a powerful algorithm suitable for various applications, especially in scenarios with smaller, low-dimensional datasets. Understanding its strengths and weaknesses is crucial for effectively utilizing KNN in real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
