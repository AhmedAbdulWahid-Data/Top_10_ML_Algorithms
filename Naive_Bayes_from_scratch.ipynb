{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier from Scratch\n",
    "\n",
    "## Overview üìä\n",
    "In this project, I‚Äôm implementing a **Naive Bayes** classifier from scratch using Python, demonstrating how this probabilistic model works under the hood. The model uses **Bayes' Theorem** to predict the class of a given sample based on conditional probabilities.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Naive Bayes Classifier**: A classification algorithm based on applying Bayes' Theorem with strong (naive) independence assumptions between the features.\n",
    "  \n",
    "- **Bayes' Theorem**: A principle that relates current evidence to prior beliefs, providing a way to calculate posterior probabilities.\n",
    "  \n",
    "- **Conditional Probability**: The likelihood of an event occurring given that another event has occurred.\n",
    "  \n",
    "- **Maximum Likelihood Estimation (MLE)**: A method for estimating the parameters of a statistical model that maximizes the likelihood of the observed data.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective üéØ\n",
    "The goal of this project is to:\n",
    "1. Implement a custom Naive Bayes classifier.\n",
    "   \n",
    "2. Calculate prior probabilities and likelihoods for each class and feature.\n",
    "   \n",
    "3. Make predictions by computing posterior probabilities and selecting the class with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "## Naive Bayes Explanation üß†\n",
    "\n",
    "### Bayes' Theorem\n",
    "Naive Bayes classifiers rely on **Bayes' Theorem** to calculate the posterior probability of a class given the features:\n",
    "\n",
    "$$\n",
    "P(C | X) = \\frac{P(X | C) P(C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P(C | X) \\): The probability of class \\(C\\) given features \\(X\\) (posterior probability).\n",
    "- \\( P(X | C) \\): The likelihood of observing the features \\(X\\) given class \\(C\\).\n",
    "- \\( P(C) \\): The prior probability of class \\(C\\).\n",
    "- \\( P(X) \\): The probability of the features \\(X\\) (also called the evidence, but usually constant in classification).\n",
    "\n",
    "### Naive Assumption\n",
    "In Naive Bayes, we assume that the features \\(X = (x_1, x_2, ..., x_n)\\) are **conditionally independent** given the class \\(C\\). This simplifies the likelihood calculation:\n",
    "\n",
    "$$\n",
    "P(X | C) = P(x_1 | C) \\cdot P(x_2 | C) \\cdot ... \\cdot P(x_n | C)\n",
    "$$\n",
    "\n",
    "Thus, the posterior probability becomes:\n",
    "\n",
    "$$\n",
    "P(C | X) = \\frac{P(C) \\prod_{i=1}^{n} P(x_i | C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P(C) \\) is the prior probability of the class.\n",
    "- \\( P(x_i | C) \\) is the likelihood of each feature \\( x_i \\) given the class \\( C \\).\n",
    "\n",
    "### Calculating the Prior and Likelihood\n",
    "\n",
    "1. **Prior Probability \\( P(C) \\)**: The prior probability of a class is the proportion of samples in that class in the dataset.\n",
    "\n",
    "   $$ P(C) = \\frac{\\text{count of class C}}{\\text{total number of samples}} $$\n",
    "\n",
    "2. **Likelihood \\( P(x_i | C) \\)**: The likelihood of each feature given the class is calculated using the frequency of feature values in the dataset.\n",
    "\n",
    "   $$ P(x_i | C) = \\frac{\\text{count of feature value } x_i \\text{ in class C}}{\\text{count of class C}} $$\n",
    "\n",
    "> Note: In case of categorical features, we treat them as counts and apply **Laplace smoothing** to avoid zero probabilities for unseen feature values.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation üõ†Ô∏è\n",
    "\n",
    "Below is the code for implementing the Naive Bayes classifier from scratch. The `NaiveBayes` class includes methods to:\n",
    "1. **Fit the model**: Learn prior probabilities and likelihoods from the training data.\n",
    "   \n",
    "2. **Predict**: Make predictions on new data based on Bayes' Theorem.\n",
    "   \n",
    "3. **Calculate Accuracy**: Evaluate the model‚Äôs accuracy on a test dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Model Fitting and Prediction\n",
    "\n",
    "1. **Training the Model**: The `fit` method learns the prior probabilities \\(P(C)\\) and likelihoods \\(P(x_i | C)\\) for each feature \\(x_i\\) in the dataset. These probabilities are stored for later use in predictions.\n",
    "\n",
    "2. **Making Predictions**: For a given test sample, the `predict` method calculates the posterior probability for each class and chooses the class with the highest probability.\n",
    "\n",
    "   $$ \\hat{y} = \\arg\\max_C P(C) \\prod_{i=1}^{n} P(x_i | C) $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's code Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Income</th>\n",
       "      <th>Education</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Young</td>\n",
       "      <td>Low</td>\n",
       "      <td>High School</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Middle-aged</td>\n",
       "      <td>High</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Old</td>\n",
       "      <td>High</td>\n",
       "      <td>Master</td>\n",
       "      <td>Don't Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Young</td>\n",
       "      <td>Low</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Middle-aged</td>\n",
       "      <td>High</td>\n",
       "      <td>Master</td>\n",
       "      <td>Buy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age Income    Education      Class\n",
       "0        Young    Low  High School        Buy\n",
       "1  Middle-aged   High     Bachelor        Buy\n",
       "2          Old   High       Master  Don't Buy\n",
       "3        Young    Low     Bachelor        Buy\n",
       "4  Middle-aged   High       Master        Buy"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generating a simple dataset\n",
    "np.random.seed(0)\n",
    "\n",
    "# Features: Age (Young, Middle-aged, Old), Income (Low, High), Education (High School, Bachelor, Master)\n",
    "data = {\n",
    "    'Age': ['Young', 'Middle-aged', 'Old', 'Young', 'Middle-aged', 'Old', 'Young', 'Middle-aged', 'Old', 'Young'],\n",
    "    'Income': ['Low', 'High', 'High', 'Low', 'High', 'Low', 'High', 'Low', 'Low', 'High'],\n",
    "    'Education': ['High School', 'Bachelor', 'Master', 'Bachelor', 'Master', 'High School', 'High School', 'Master', 'Bachelor', 'Master'],\n",
    "    'Class': ['Buy', 'Buy', 'Don\\'t Buy', 'Buy', 'Buy', 'Don\\'t Buy', 'Buy', 'Don\\'t Buy', 'Don\\'t Buy', 'Buy']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Show the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        # Initialize dictionaries to store prior probabilities and likelihoods for each class\n",
    "        self.prior_probs = {}  # Dictionary to store P(C) - Prior probability for each class\n",
    "        self.likelihoods = {}  # Dictionary to store P(x_i | C) - Likelihood of each feature value given a class\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Naive Bayes model by calculating prior probabilities and likelihoods.\n",
    "        \n",
    "        X: DataFrame of input features (independent variables)\n",
    "        y: Series of target labels (dependent variable)\n",
    "        \"\"\"\n",
    "        total_samples = len(y)  # Get the total number of samples in the dataset\n",
    "        \n",
    "        # Calculate prior probabilities P(C), which is the frequency of each class divided by the total number of samples\n",
    "        self.prior_probs = y.value_counts() / total_samples\n",
    "        \n",
    "        # Initialize the likelihoods dictionary to store the probability of feature values given a class\n",
    "        self.likelihoods = {}\n",
    "        \n",
    "        # Loop through each unique class in the target variable\n",
    "        for c in y.unique():\n",
    "            # Get the subset of data where the target variable equals the current class\n",
    "            class_data = X[y == c]\n",
    "            \n",
    "            # Initialize a dictionary for each feature in the current class\n",
    "            self.likelihoods[c] = {}\n",
    "            \n",
    "            # Loop through each feature column in the input data\n",
    "            for feature in X.columns:\n",
    "                # Calculate the probability of each feature value in this class using value_counts\n",
    "                feature_values = class_data[feature].value_counts() / len(class_data)\n",
    "                \n",
    "                # Store the calculated probabilities for the current feature and class\n",
    "                self.likelihoods[c][feature] = feature_values\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions for the given input features based on the trained model.\n",
    "        \n",
    "        X: DataFrame of input features (independent variables) to classify\n",
    "        \"\"\"\n",
    "        predictions = []  # List to store the predicted classes for each row\n",
    "        \n",
    "        # Loop through each row of input features in X to make predictions\n",
    "        for _, row in X.iterrows():\n",
    "            posteriors = {}  # Dictionary to store the calculated posterior probability for each class\n",
    "            \n",
    "            # Loop through each class and calculate its posterior probability\n",
    "            for c in self.prior_probs.index:\n",
    "                # Start with the prior probability P(C) for the class, using log to handle small values\n",
    "                posterior = np.log(self.prior_probs[c])\n",
    "                \n",
    "                # Loop through each feature to calculate its contribution to the posterior probability\n",
    "                for feature in X.columns:\n",
    "                    feature_value = row[feature]  # Get the feature value for the current row\n",
    "                    # Get the likelihood P(x_i | C) for the feature value, with Laplace smoothing to handle zero probabilities\n",
    "                    likelihood = self.likelihoods[c].get(feature, {}).get(feature_value, 1e-6)\n",
    "                    # Add the log of the likelihood to the posterior\n",
    "                    posterior += np.log(likelihood)\n",
    "                \n",
    "                # Store the calculated posterior probability for the current class\n",
    "                posteriors[c] = posterior\n",
    "            \n",
    "            # Choose the class with the highest posterior probability and append it to predictions\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        \n",
    "        return predictions  # Return the list of predicted classes\n",
    "\n",
    "# Step 3: Training the model and making predictions\n",
    "X = df[['Age', 'Income', 'Education']]  # Select the input features\n",
    "y = df['Class']  # Select the target variable (class labels)\n",
    "\n",
    "# Convert categorical features to a numeric format (i.e., category codes) for processing by the model\n",
    "X = X.apply(lambda col: col.astype('category').cat.codes)\n",
    "\n",
    "# Train the Naive Bayes model using the input features and target variable\n",
    "nb = NaiveBayes()  # Instantiate the NaiveBayes class\n",
    "nb.fit(X, y)  # Fit the model to the data\n",
    "\n",
    "# Step 4: Making predictions using the trained model\n",
    "predictions = nb.predict(X)  # Get the predicted classes for the input features\n",
    "\n",
    "# Step 5: Evaluating the model by calculating the accuracy\n",
    "accuracy = sum(predictions == y) / len(y)  # Calculate accuracy as the percentage of correct predictions\n",
    "print(f\"Accuracy: {accuracy:.4f}\")  # Print the accuracy with 4 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When to Use Naive Bayes üìà\n",
    "\n",
    "- **Text Classification**: Great for tasks like spam detection and sentiment analysis, where features (e.g., words) are abundant.\n",
    "\n",
    "- **Binary and Multi-Class Classification**: Works well for both yes/no and multi-category problems.\n",
    "  \n",
    "- **Independent Features**: Best when features are independent or weakly dependent.\n",
    "  \n",
    "- **Large Datasets**: Efficient for big datasets with categorical features.\n",
    "  \n",
    "- **Real-Time Prediction**: Ideal for fast predictions in systems like recommendation engines.\n",
    "\n",
    "---\n",
    "\n",
    "# Pros of Naive Bayes ‚úÖ\n",
    "\n",
    "- **Simple and Fast**: Easy to implement and quick to train and predict.\n",
    "  \n",
    "- **Works with Large Datasets**: Efficient for big data.\n",
    "  \n",
    "- **Good with Categorical Features**: Perfect for tasks like text classification.\n",
    "  \n",
    "- **Handles Missing Data**: Ignores missing values during calculation.\n",
    "  \n",
    "- **Effective for Text Data**: Great for high-dimensional data like words in documents.\n",
    "\n",
    "---\n",
    "\n",
    "# Cons of Naive Bayes ‚ùå\n",
    "\n",
    "- **Independence Assumption**: Struggles with correlated features, as it assumes all features are independent.\n",
    "  \n",
    "- **Not Great for Correlated Features**: Performs poorly if features are strongly related.\n",
    "  \n",
    "- **Limited Complexity**: Simple model, so it may miss complex patterns.\n",
    "  \n",
    "- **Issues with Continuous Data**: Needs transformation (e.g., Gaussian distribution) to handle continuous features.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion üéØ\n",
    "\n",
    "Naive Bayes is fast, simple, and efficient for tasks like text classification, large datasets with categorical data, and real-time predictions. However, its assumption of feature independence can limit its performance when features are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
